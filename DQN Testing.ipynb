{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the relevant variables:\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions in Cartpole game\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build a simple hidden layer\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   79/5000: episode: 1, duration: 2.793s, episode steps: 79, steps per second: 28, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.428186, mean_absolute_error: 0.496044, mean_q: 0.053062\n",
      "  113/5000: episode: 2, duration: 0.568s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.159, 0.753], loss: 0.351612, mean_absolute_error: 0.445402, mean_q: 0.193012\n",
      "  165/5000: episode: 3, duration: 0.865s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.088 [-0.295, 0.673], loss: 0.314531, mean_absolute_error: 0.467520, mean_q: 0.322337\n",
      "  199/5000: episode: 4, duration: 0.568s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.081 [-0.228, 0.770], loss: 0.267528, mean_absolute_error: 0.496035, mean_q: 0.481354\n",
      "  258/5000: episode: 5, duration: 0.980s, episode steps: 59, steps per second: 60, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.016 [-0.371, 0.844], loss: 0.229219, mean_absolute_error: 0.557385, mean_q: 0.681756\n",
      "  292/5000: episode: 6, duration: 0.567s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.207, 0.836], loss: 0.178329, mean_absolute_error: 0.630491, mean_q: 0.928109\n",
      "  321/5000: episode: 7, duration: 0.482s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.077 [-0.543, 1.061], loss: 0.154149, mean_absolute_error: 0.701702, mean_q: 1.111808\n",
      "  348/5000: episode: 8, duration: 0.450s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.081 [-0.408, 0.879], loss: 0.115736, mean_absolute_error: 0.760415, mean_q: 1.313146\n",
      "  368/5000: episode: 9, duration: 0.331s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.547, 1.039], loss: 0.105325, mean_absolute_error: 0.837844, mean_q: 1.482533\n",
      "  386/5000: episode: 10, duration: 0.299s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.427, 1.096], loss: 0.097229, mean_absolute_error: 0.909514, mean_q: 1.651678\n",
      "  409/5000: episode: 11, duration: 0.384s, episode steps: 23, steps per second: 60, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.079 [-0.428, 0.889], loss: 0.087648, mean_absolute_error: 0.978099, mean_q: 1.823476\n",
      "  430/5000: episode: 12, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.089 [-0.369, 1.041], loss: 0.083056, mean_absolute_error: 1.051278, mean_q: 1.992207\n",
      "  454/5000: episode: 13, duration: 0.398s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.085 [-0.544, 1.087], loss: 0.091281, mean_absolute_error: 1.155828, mean_q: 2.194736\n",
      "  479/5000: episode: 14, duration: 0.416s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.081 [-0.396, 1.101], loss: 0.098444, mean_absolute_error: 1.251338, mean_q: 2.385940\n",
      "  493/5000: episode: 15, duration: 0.238s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.086 [-0.605, 1.203], loss: 0.122502, mean_absolute_error: 1.322281, mean_q: 2.538469\n",
      "  505/5000: episode: 16, duration: 0.195s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.116 [-0.773, 1.216], loss: 0.108166, mean_absolute_error: 1.365949, mean_q: 2.626013\n",
      "  523/5000: episode: 17, duration: 0.299s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.071 [-0.582, 1.185], loss: 0.136003, mean_absolute_error: 1.443932, mean_q: 2.793187\n",
      "  541/5000: episode: 18, duration: 0.299s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.083 [-0.610, 1.213], loss: 0.127734, mean_absolute_error: 1.523100, mean_q: 2.966722\n",
      "  554/5000: episode: 19, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.120 [-0.766, 1.407], loss: 0.139524, mean_absolute_error: 1.569396, mean_q: 3.111924\n",
      "  565/5000: episode: 20, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-0.784, 1.384], loss: 0.217489, mean_absolute_error: 1.678441, mean_q: 3.227309\n",
      "  574/5000: episode: 21, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.144 [-0.957, 1.562], loss: 0.295481, mean_absolute_error: 1.732787, mean_q: 3.317744\n",
      "  586/5000: episode: 22, duration: 0.197s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-0.805, 1.493], loss: 0.214742, mean_absolute_error: 1.743347, mean_q: 3.397157\n",
      "  595/5000: episode: 23, duration: 0.151s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.149 [-1.141, 1.969], loss: 0.265282, mean_absolute_error: 1.841232, mean_q: 3.535380\n",
      "  604/5000: episode: 24, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.153 [-1.550, 2.453], loss: 0.400617, mean_absolute_error: 1.905844, mean_q: 3.643163\n",
      "  614/5000: episode: 25, duration: 0.197s, episode steps: 10, steps per second: 51, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.105 [-1.579, 2.449], loss: 0.276818, mean_absolute_error: 1.921971, mean_q: 3.705682\n",
      "  622/5000: episode: 26, duration: 0.135s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.551, 2.549], loss: 0.240495, mean_absolute_error: 1.897776, mean_q: 3.775074\n",
      "  632/5000: episode: 27, duration: 0.199s, episode steps: 10, steps per second: 50, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.964, 3.004], loss: 0.312104, mean_absolute_error: 2.000367, mean_q: 3.924464\n",
      "  641/5000: episode: 28, duration: 0.165s, episode steps: 9, steps per second: 55, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.810, 2.873], loss: 0.442270, mean_absolute_error: 2.096843, mean_q: 4.019598\n",
      "  651/5000: episode: 29, duration: 0.232s, episode steps: 10, steps per second: 43, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.161 [-1.521, 2.531], loss: 0.432390, mean_absolute_error: 2.157328, mean_q: 4.069087\n",
      "  662/5000: episode: 30, duration: 0.216s, episode steps: 11, steps per second: 51, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-1.133, 1.936], loss: 0.454989, mean_absolute_error: 2.165499, mean_q: 4.096240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  675/5000: episode: 31, duration: 0.238s, episode steps: 13, steps per second: 55, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.087 [-1.207, 1.871], loss: 0.423647, mean_absolute_error: 2.193084, mean_q: 4.225003\n",
      "  684/5000: episode: 32, duration: 0.165s, episode steps: 9, steps per second: 54, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.124 [-1.418, 2.297], loss: 0.355577, mean_absolute_error: 2.276168, mean_q: 4.358374\n",
      "  694/5000: episode: 33, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.144 [-1.348, 2.185], loss: 0.334126, mean_absolute_error: 2.305633, mean_q: 4.487020\n",
      "  703/5000: episode: 34, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.140 [-1.319, 2.165], loss: 0.307051, mean_absolute_error: 2.321749, mean_q: 4.572862\n",
      "  712/5000: episode: 35, duration: 0.167s, episode steps: 9, steps per second: 54, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.141 [-1.201, 1.986], loss: 0.649086, mean_absolute_error: 2.444367, mean_q: 4.666613\n",
      "  723/5000: episode: 36, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.115 [-1.395, 2.243], loss: 0.407297, mean_absolute_error: 2.407176, mean_q: 4.668962\n",
      "  734/5000: episode: 37, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.126 [-1.355, 2.194], loss: 0.509368, mean_absolute_error: 2.490329, mean_q: 4.801661\n",
      "  744/5000: episode: 38, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.114 [-1.351, 2.209], loss: 0.297231, mean_absolute_error: 2.498477, mean_q: 4.891623\n",
      "  755/5000: episode: 39, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.122 [-1.376, 2.231], loss: 0.512954, mean_absolute_error: 2.559643, mean_q: 5.039338\n",
      "  764/5000: episode: 40, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.384, 2.243], loss: 0.720780, mean_absolute_error: 2.634284, mean_q: 5.090855\n",
      "  772/5000: episode: 41, duration: 0.135s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.157 [-1.175, 2.000], loss: 0.544244, mean_absolute_error: 2.648175, mean_q: 5.109720\n",
      "  781/5000: episode: 42, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.118 [-1.420, 2.228], loss: 0.605154, mean_absolute_error: 2.734200, mean_q: 5.177201\n",
      "  791/5000: episode: 43, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.149 [-1.142, 2.059], loss: 0.756316, mean_absolute_error: 2.803196, mean_q: 5.286414\n",
      "  801/5000: episode: 44, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.330, 2.143], loss: 0.591978, mean_absolute_error: 2.788139, mean_q: 5.334446\n",
      "  810/5000: episode: 45, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.156 [-1.332, 2.266], loss: 0.533108, mean_absolute_error: 2.820461, mean_q: 5.461889\n",
      "  819/5000: episode: 46, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.170 [-1.338, 2.318], loss: 0.892184, mean_absolute_error: 2.958100, mean_q: 5.562053\n",
      "  828/5000: episode: 47, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.154 [-1.332, 2.211], loss: 0.612979, mean_absolute_error: 2.908586, mean_q: 5.597309\n",
      "  838/5000: episode: 48, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.113 [-1.212, 1.985], loss: 0.619212, mean_absolute_error: 2.932408, mean_q: 5.603985\n",
      "  848/5000: episode: 49, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.186, 1.988], loss: 0.961878, mean_absolute_error: 3.058206, mean_q: 5.689036\n",
      "  859/5000: episode: 50, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.089 [-1.196, 1.846], loss: 0.761849, mean_absolute_error: 3.071951, mean_q: 5.751876\n",
      "  869/5000: episode: 51, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.144, 1.941], loss: 0.561662, mean_absolute_error: 3.044773, mean_q: 5.837141\n",
      "  881/5000: episode: 52, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.108 [-1.211, 1.943], loss: 0.965481, mean_absolute_error: 3.148076, mean_q: 5.922215\n",
      "  894/5000: episode: 53, duration: 0.214s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.105 [-1.026, 1.698], loss: 0.740940, mean_absolute_error: 3.150095, mean_q: 5.936515\n",
      "  906/5000: episode: 54, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.094 [-0.987, 1.618], loss: 0.829984, mean_absolute_error: 3.214526, mean_q: 6.025727\n",
      "  916/5000: episode: 55, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.119 [-0.963, 1.645], loss: 0.744062, mean_absolute_error: 3.210393, mean_q: 6.135869\n",
      "  928/5000: episode: 56, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.095 [-1.200, 1.753], loss: 0.762330, mean_absolute_error: 3.265318, mean_q: 6.183347\n",
      "  940/5000: episode: 57, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-0.807, 1.495], loss: 0.835064, mean_absolute_error: 3.309803, mean_q: 6.254766\n",
      "  951/5000: episode: 58, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.112 [-0.952, 1.596], loss: 0.974278, mean_absolute_error: 3.393936, mean_q: 6.311846\n",
      "  960/5000: episode: 59, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.114 [-1.019, 1.644], loss: 0.936428, mean_absolute_error: 3.399442, mean_q: 6.373389\n",
      "  971/5000: episode: 60, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.111 [-1.026, 1.683], loss: 0.905609, mean_absolute_error: 3.446699, mean_q: 6.455338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  983/5000: episode: 61, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.083 [-1.184, 1.769], loss: 1.060042, mean_absolute_error: 3.501330, mean_q: 6.475317\n",
      "  997/5000: episode: 62, duration: 0.236s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.092 [-0.788, 1.489], loss: 0.833642, mean_absolute_error: 3.500277, mean_q: 6.528677\n",
      " 1013/5000: episode: 63, duration: 0.269s, episode steps: 16, steps per second: 59, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.093 [-0.944, 1.642], loss: 0.960523, mean_absolute_error: 3.573193, mean_q: 6.695931\n",
      " 1026/5000: episode: 64, duration: 0.210s, episode steps: 13, steps per second: 62, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.105 [-0.937, 1.532], loss: 0.974370, mean_absolute_error: 3.605494, mean_q: 6.749664\n",
      " 1041/5000: episode: 65, duration: 0.261s, episode steps: 15, steps per second: 58, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.072 [-0.797, 1.223], loss: 1.136084, mean_absolute_error: 3.675524, mean_q: 6.758059\n",
      " 1055/5000: episode: 66, duration: 0.226s, episode steps: 14, steps per second: 62, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.098 [-0.771, 1.360], loss: 1.285960, mean_absolute_error: 3.724346, mean_q: 6.787498\n",
      " 1070/5000: episode: 67, duration: 0.265s, episode steps: 15, steps per second: 57, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.795, 1.405], loss: 1.398901, mean_absolute_error: 3.764245, mean_q: 6.834656\n",
      " 1081/5000: episode: 68, duration: 0.160s, episode steps: 11, steps per second: 69, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.122 [-0.769, 1.425], loss: 1.144502, mean_absolute_error: 3.768898, mean_q: 6.907719\n",
      " 1095/5000: episode: 69, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.091 [-0.772, 1.278], loss: 1.004861, mean_absolute_error: 3.758170, mean_q: 7.000823\n",
      " 1109/5000: episode: 70, duration: 0.245s, episode steps: 14, steps per second: 57, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.118 [-0.779, 1.503], loss: 1.077092, mean_absolute_error: 3.818949, mean_q: 7.046448\n",
      " 1119/5000: episode: 71, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.114 [-0.832, 1.355], loss: 1.645029, mean_absolute_error: 3.934001, mean_q: 7.116696\n",
      " 1130/5000: episode: 72, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.134 [-0.762, 1.493], loss: 1.021207, mean_absolute_error: 3.868573, mean_q: 7.121583\n",
      " 1143/5000: episode: 73, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.086 [-0.835, 1.432], loss: 1.444975, mean_absolute_error: 3.952315, mean_q: 7.137306\n",
      " 1153/5000: episode: 74, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.135 [-0.753, 1.379], loss: 1.227656, mean_absolute_error: 3.929832, mean_q: 7.155338\n",
      " 1164/5000: episode: 75, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.111 [-0.768, 1.373], loss: 1.292063, mean_absolute_error: 3.954868, mean_q: 7.179870\n",
      " 1176/5000: episode: 76, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.136 [-0.750, 1.486], loss: 1.794141, mean_absolute_error: 4.049488, mean_q: 7.152480\n",
      " 1191/5000: episode: 77, duration: 0.250s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.765, 1.356], loss: 1.111616, mean_absolute_error: 3.986354, mean_q: 7.301120\n",
      " 1201/5000: episode: 78, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.132 [-0.750, 1.461], loss: 1.313877, mean_absolute_error: 4.039087, mean_q: 7.379809\n",
      " 1214/5000: episode: 79, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.114 [-0.757, 1.457], loss: 1.212499, mean_absolute_error: 4.021295, mean_q: 7.329585\n",
      " 1228/5000: episode: 80, duration: 0.236s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.103 [-0.741, 1.252], loss: 1.210822, mean_absolute_error: 4.071729, mean_q: 7.503515\n",
      " 1245/5000: episode: 81, duration: 0.279s, episode steps: 17, steps per second: 61, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.089 [-0.583, 1.154], loss: 1.251189, mean_absolute_error: 4.085403, mean_q: 7.443412\n",
      " 1262/5000: episode: 82, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.087 [-0.559, 1.249], loss: 1.109830, mean_absolute_error: 4.115475, mean_q: 7.614866\n",
      " 1278/5000: episode: 83, duration: 0.272s, episode steps: 16, steps per second: 59, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.087 [-0.622, 1.203], loss: 1.057214, mean_absolute_error: 4.153069, mean_q: 7.711330\n",
      " 1293/5000: episode: 84, duration: 0.243s, episode steps: 15, steps per second: 62, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-0.558, 1.253], loss: 1.337004, mean_absolute_error: 4.190986, mean_q: 7.724144\n",
      " 1308/5000: episode: 85, duration: 0.272s, episode steps: 15, steps per second: 55, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.101 [-0.602, 1.021], loss: 1.458780, mean_absolute_error: 4.228430, mean_q: 7.615014\n",
      " 1325/5000: episode: 86, duration: 0.277s, episode steps: 17, steps per second: 61, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.087 [-0.577, 0.967], loss: 1.064183, mean_absolute_error: 4.230179, mean_q: 7.758653\n",
      " 1355/5000: episode: 87, duration: 0.482s, episode steps: 30, steps per second: 62, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.347, 0.739], loss: 1.177764, mean_absolute_error: 4.307146, mean_q: 7.956305\n",
      " 1408/5000: episode: 88, duration: 0.902s, episode steps: 53, steps per second: 59, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.059 [-0.423, 0.821], loss: 1.207577, mean_absolute_error: 4.376864, mean_q: 8.067945\n",
      " 1446/5000: episode: 89, duration: 0.630s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.415, 0.896], loss: 1.212505, mean_absolute_error: 4.470487, mean_q: 8.243876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1491/5000: episode: 90, duration: 0.783s, episode steps: 45, steps per second: 57, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.096 [-0.544, 0.818], loss: 1.311603, mean_absolute_error: 4.562609, mean_q: 8.422343\n",
      " 1540/5000: episode: 91, duration: 0.833s, episode steps: 49, steps per second: 59, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.120 [-0.759, 0.259], loss: 1.364419, mean_absolute_error: 4.674416, mean_q: 8.651674\n",
      " 1574/5000: episode: 92, duration: 0.466s, episode steps: 34, steps per second: 73, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-0.964, 0.221], loss: 1.460110, mean_absolute_error: 4.786120, mean_q: 8.817448\n",
      " 1598/5000: episode: 93, duration: 0.557s, episode steps: 24, steps per second: 43, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-0.944, 0.392], loss: 1.002984, mean_absolute_error: 4.784237, mean_q: 9.049991\n",
      " 1625/5000: episode: 94, duration: 0.534s, episode steps: 27, steps per second: 51, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.090 [-0.982, 0.204], loss: 1.542921, mean_absolute_error: 4.938628, mean_q: 9.235253\n",
      " 1662/5000: episode: 95, duration: 1.157s, episode steps: 37, steps per second: 32, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.030 [-0.962, 0.427], loss: 1.558422, mean_absolute_error: 5.020965, mean_q: 9.432653\n",
      " 1683/5000: episode: 96, duration: 0.460s, episode steps: 21, steps per second: 46, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.066 [-1.345, 0.651], loss: 1.184014, mean_absolute_error: 5.097528, mean_q: 9.660582\n",
      " 1696/5000: episode: 97, duration: 0.558s, episode steps: 13, steps per second: 23, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.101 [-1.341, 0.774], loss: 1.223010, mean_absolute_error: 5.111293, mean_q: 9.703533\n",
      " 1729/5000: episode: 98, duration: 1.080s, episode steps: 33, steps per second: 31, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.033 [-1.337, 0.635], loss: 1.194291, mean_absolute_error: 5.250197, mean_q: 10.041149\n",
      " 1750/5000: episode: 99, duration: 0.724s, episode steps: 21, steps per second: 29, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.078 [-1.307, 0.595], loss: 1.348245, mean_absolute_error: 5.331667, mean_q: 10.110072\n",
      " 1763/5000: episode: 100, duration: 0.454s, episode steps: 13, steps per second: 29, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.137 [-1.315, 0.561], loss: 1.319820, mean_absolute_error: 5.378245, mean_q: 10.200494\n",
      " 1780/5000: episode: 101, duration: 0.532s, episode steps: 17, steps per second: 32, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.110 [-1.070, 0.418], loss: 1.274471, mean_absolute_error: 5.410131, mean_q: 10.338453\n",
      " 1795/5000: episode: 102, duration: 0.443s, episode steps: 15, steps per second: 34, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.109 [-1.054, 0.580], loss: 1.286747, mean_absolute_error: 5.448886, mean_q: 10.437677\n",
      " 1810/5000: episode: 103, duration: 0.463s, episode steps: 15, steps per second: 32, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.089 [-1.015, 0.417], loss: 1.936160, mean_absolute_error: 5.579252, mean_q: 10.536782\n",
      " 1831/5000: episode: 104, duration: 0.518s, episode steps: 21, steps per second: 41, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.069 [-1.268, 0.787], loss: 2.025226, mean_absolute_error: 5.631689, mean_q: 10.584401\n",
      " 1844/5000: episode: 105, duration: 0.329s, episode steps: 13, steps per second: 40, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.110 [-1.277, 0.584], loss: 1.994013, mean_absolute_error: 5.665105, mean_q: 10.623728\n",
      " 1861/5000: episode: 106, duration: 0.499s, episode steps: 17, steps per second: 34, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.106 [-1.061, 0.404], loss: 1.804338, mean_absolute_error: 5.648593, mean_q: 10.588866\n",
      " 1887/5000: episode: 107, duration: 0.735s, episode steps: 26, steps per second: 35, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-0.942, 0.404], loss: 1.450109, mean_absolute_error: 5.726545, mean_q: 10.870376\n",
      " 1913/5000: episode: 108, duration: 0.688s, episode steps: 26, steps per second: 38, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-0.921, 0.435], loss: 1.923898, mean_absolute_error: 5.798058, mean_q: 10.883133\n",
      " 1949/5000: episode: 109, duration: 0.948s, episode steps: 36, steps per second: 38, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-0.787, 0.615], loss: 1.874710, mean_absolute_error: 5.891290, mean_q: 11.116641\n",
      " 1971/5000: episode: 110, duration: 0.619s, episode steps: 22, steps per second: 36, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.099 [-1.215, 0.590], loss: 2.047669, mean_absolute_error: 5.928943, mean_q: 11.182980\n",
      " 1993/5000: episode: 111, duration: 0.616s, episode steps: 22, steps per second: 36, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-0.842, 0.386], loss: 1.946130, mean_absolute_error: 6.010791, mean_q: 11.333134\n",
      " 2022/5000: episode: 112, duration: 1.009s, episode steps: 29, steps per second: 29, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.087 [-0.796, 0.274], loss: 1.783342, mean_absolute_error: 6.073669, mean_q: 11.523190\n",
      " 2054/5000: episode: 113, duration: 1.043s, episode steps: 32, steps per second: 31, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.044 [-0.909, 0.423], loss: 2.228759, mean_absolute_error: 6.198031, mean_q: 11.754463\n",
      " 2081/5000: episode: 114, duration: 0.826s, episode steps: 27, steps per second: 33, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.079 [-1.058, 0.378], loss: 1.646616, mean_absolute_error: 6.230198, mean_q: 11.956746\n",
      " 2107/5000: episode: 115, duration: 0.767s, episode steps: 26, steps per second: 34, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-0.859, 0.367], loss: 2.332519, mean_absolute_error: 6.349403, mean_q: 11.987336\n",
      " 2133/5000: episode: 116, duration: 0.755s, episode steps: 26, steps per second: 34, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-0.884, 0.376], loss: 2.872776, mean_absolute_error: 6.473439, mean_q: 12.144490\n",
      " 2152/5000: episode: 117, duration: 0.579s, episode steps: 19, steps per second: 33, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.110 [-1.009, 0.402], loss: 1.865770, mean_absolute_error: 6.378217, mean_q: 12.123413\n",
      " 2170/5000: episode: 118, duration: 0.520s, episode steps: 18, steps per second: 35, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-0.830, 0.373], loss: 1.417866, mean_absolute_error: 6.431722, mean_q: 12.395023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2184/5000: episode: 119, duration: 0.414s, episode steps: 14, steps per second: 34, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.116 [-1.197, 0.608], loss: 2.000751, mean_absolute_error: 6.568384, mean_q: 12.614542\n",
      " 2199/5000: episode: 120, duration: 0.473s, episode steps: 15, steps per second: 32, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.102 [-1.071, 0.454], loss: 3.439864, mean_absolute_error: 6.669377, mean_q: 12.565231\n",
      " 2219/5000: episode: 121, duration: 0.644s, episode steps: 20, steps per second: 31, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-0.889, 0.364], loss: 2.519728, mean_absolute_error: 6.639435, mean_q: 12.573806\n",
      " 2280/5000: episode: 122, duration: 1.924s, episode steps: 61, steps per second: 32, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.004 [-0.887, 0.387], loss: 2.630423, mean_absolute_error: 6.712084, mean_q: 12.620353\n",
      " 2297/5000: episode: 123, duration: 0.535s, episode steps: 17, steps per second: 32, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.122 [-0.884, 0.361], loss: 2.264182, mean_absolute_error: 6.748849, mean_q: 12.822517\n",
      " 2344/5000: episode: 124, duration: 1.427s, episode steps: 47, steps per second: 33, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.005 [-0.926, 0.590], loss: 2.502959, mean_absolute_error: 6.873079, mean_q: 13.049977\n",
      " 2368/5000: episode: 125, duration: 0.574s, episode steps: 24, steps per second: 42, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.128, 0.410], loss: 2.098390, mean_absolute_error: 6.891155, mean_q: 13.186046\n",
      " 2390/5000: episode: 126, duration: 0.412s, episode steps: 22, steps per second: 53, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-0.805, 0.241], loss: 2.984332, mean_absolute_error: 6.970676, mean_q: 13.127832\n",
      " 2417/5000: episode: 127, duration: 0.462s, episode steps: 27, steps per second: 58, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.100 [-0.814, 0.252], loss: 2.837181, mean_absolute_error: 7.137671, mean_q: 13.486340\n",
      " 2518/5000: episode: 128, duration: 1.699s, episode steps: 101, steps per second: 59, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.031 [-0.875, 0.621], loss: 3.044214, mean_absolute_error: 7.190612, mean_q: 13.638835\n",
      " 2580/5000: episode: 129, duration: 1.048s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.089 [-0.297, 0.816], loss: 2.613602, mean_absolute_error: 7.331418, mean_q: 13.988557\n",
      " 2646/5000: episode: 130, duration: 1.216s, episode steps: 66, steps per second: 54, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.037 [-0.796, 0.365], loss: 2.978693, mean_absolute_error: 7.491028, mean_q: 14.286274\n",
      " 2686/5000: episode: 131, duration: 0.682s, episode steps: 40, steps per second: 59, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.112 [-0.709, 0.183], loss: 3.278362, mean_absolute_error: 7.560597, mean_q: 14.331141\n",
      " 2750/5000: episode: 132, duration: 1.068s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-1.080, 0.403], loss: 2.930742, mean_absolute_error: 7.708380, mean_q: 14.711220\n",
      " 2807/5000: episode: 133, duration: 0.949s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.057 [-0.699, 0.426], loss: 2.472713, mean_absolute_error: 7.716958, mean_q: 14.809881\n",
      " 2845/5000: episode: 134, duration: 0.632s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.099 [-0.708, 0.223], loss: 3.140306, mean_absolute_error: 7.878355, mean_q: 15.074887\n",
      " 2878/5000: episode: 135, duration: 0.550s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.114 [-0.789, 0.235], loss: 3.564232, mean_absolute_error: 7.961782, mean_q: 15.106404\n",
      " 2923/5000: episode: 136, duration: 0.749s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.091 [-1.018, 0.349], loss: 3.123319, mean_absolute_error: 7.990331, mean_q: 15.265187\n",
      " 2968/5000: episode: 137, duration: 0.749s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.119 [-0.718, 0.395], loss: 2.779632, mean_absolute_error: 8.038029, mean_q: 15.414820\n",
      " 3014/5000: episode: 138, duration: 0.765s, episode steps: 46, steps per second: 60, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.101 [-0.859, 0.222], loss: 3.507472, mean_absolute_error: 8.121001, mean_q: 15.484862\n",
      " 3071/5000: episode: 139, duration: 0.950s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.074 [-0.827, 0.320], loss: 2.958983, mean_absolute_error: 8.185517, mean_q: 15.741837\n",
      " 3106/5000: episode: 140, duration: 0.583s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.131 [-0.652, 0.204], loss: 3.498117, mean_absolute_error: 8.342663, mean_q: 15.971090\n",
      " 3156/5000: episode: 141, duration: 0.831s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.067 [-0.919, 0.287], loss: 3.225006, mean_absolute_error: 8.372103, mean_q: 16.097414\n",
      " 3185/5000: episode: 142, duration: 0.484s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.132 [-0.609, 0.191], loss: 2.649803, mean_absolute_error: 8.444838, mean_q: 16.316622\n",
      " 3225/5000: episode: 143, duration: 0.666s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.133 [-0.416, 0.780], loss: 3.943019, mean_absolute_error: 8.536402, mean_q: 16.252405\n",
      " 3281/5000: episode: 144, duration: 0.932s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.100 [-0.696, 0.271], loss: 3.220771, mean_absolute_error: 8.562025, mean_q: 16.418022\n",
      " 3370/5000: episode: 145, duration: 1.483s, episode steps: 89, steps per second: 60, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.013 [-0.700, 0.599], loss: 3.565857, mean_absolute_error: 8.662001, mean_q: 16.610119\n",
      " 3400/5000: episode: 146, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.132 [-0.897, 0.231], loss: 3.462649, mean_absolute_error: 8.726208, mean_q: 16.738899\n",
      " 3439/5000: episode: 147, duration: 0.649s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.134 [-0.587, 0.169], loss: 3.618627, mean_absolute_error: 8.856864, mean_q: 16.967651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3510/5000: episode: 148, duration: 1.183s, episode steps: 71, steps per second: 60, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.076 [-0.716, 0.481], loss: 3.010715, mean_absolute_error: 8.853054, mean_q: 17.117516\n",
      " 3548/5000: episode: 149, duration: 0.650s, episode steps: 38, steps per second: 59, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.126 [-0.721, 0.210], loss: 3.468884, mean_absolute_error: 8.969676, mean_q: 17.296041\n",
      " 3587/5000: episode: 150, duration: 0.648s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.132 [-0.656, 0.383], loss: 2.801829, mean_absolute_error: 9.033515, mean_q: 17.556305\n",
      " 3641/5000: episode: 151, duration: 0.901s, episode steps: 54, steps per second: 60, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.102 [-0.718, 0.209], loss: 4.019021, mean_absolute_error: 9.156158, mean_q: 17.616329\n",
      " 3698/5000: episode: 152, duration: 0.949s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.112 [-0.784, 0.343], loss: 3.920513, mean_absolute_error: 9.208734, mean_q: 17.704525\n",
      " 3751/5000: episode: 153, duration: 0.902s, episode steps: 53, steps per second: 59, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.100 [-0.714, 0.304], loss: 3.584696, mean_absolute_error: 9.272463, mean_q: 17.946426\n",
      " 3794/5000: episode: 154, duration: 1.162s, episode steps: 43, steps per second: 37, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.105 [-0.910, 0.367], loss: 3.607449, mean_absolute_error: 9.335880, mean_q: 18.059435\n",
      " 3842/5000: episode: 155, duration: 0.967s, episode steps: 48, steps per second: 50, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.090 [-0.714, 0.405], loss: 3.830809, mean_absolute_error: 9.441143, mean_q: 18.197603\n",
      " 3882/5000: episode: 156, duration: 0.764s, episode steps: 40, steps per second: 52, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.136 [-0.739, 0.183], loss: 3.219791, mean_absolute_error: 9.428980, mean_q: 18.302807\n",
      " 3959/5000: episode: 157, duration: 1.350s, episode steps: 77, steps per second: 57, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.118 [-0.452, 0.966], loss: 3.560684, mean_absolute_error: 9.593846, mean_q: 18.575363\n",
      " 4008/5000: episode: 158, duration: 0.439s, episode steps: 49, steps per second: 112, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.127 [-0.718, 0.222], loss: 3.718851, mean_absolute_error: 9.615232, mean_q: 18.609266\n",
      " 4067/5000: episode: 159, duration: 0.291s, episode steps: 59, steps per second: 203, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.057 [-0.627, 0.417], loss: 3.519878, mean_absolute_error: 9.735703, mean_q: 18.812923\n",
      " 4096/5000: episode: 160, duration: 0.405s, episode steps: 29, steps per second: 72, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.092 [-0.815, 0.445], loss: 3.042019, mean_absolute_error: 9.800789, mean_q: 19.092186\n",
      " 4149/5000: episode: 161, duration: 0.898s, episode steps: 53, steps per second: 59, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.065 [-0.694, 0.469], loss: 3.457650, mean_absolute_error: 9.892441, mean_q: 19.207834\n",
      " 4216/5000: episode: 162, duration: 0.993s, episode steps: 67, steps per second: 67, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.094 [-0.885, 0.415], loss: 3.355410, mean_absolute_error: 9.953689, mean_q: 19.376677\n",
      " 4266/5000: episode: 163, duration: 0.505s, episode steps: 50, steps per second: 99, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.125 [-0.764, 0.292], loss: 4.237083, mean_absolute_error: 10.154810, mean_q: 19.677332\n",
      " 4357/5000: episode: 164, duration: 1.533s, episode steps: 91, steps per second: 59, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.064 [-1.250, 0.432], loss: 4.480817, mean_absolute_error: 10.151945, mean_q: 19.577833\n",
      " 4417/5000: episode: 165, duration: 1.069s, episode steps: 60, steps per second: 56, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.162 [-1.113, 0.174], loss: 3.285556, mean_absolute_error: 10.153193, mean_q: 19.786806\n",
      " 4508/5000: episode: 166, duration: 1.511s, episode steps: 91, steps per second: 60, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.067 [-0.969, 0.336], loss: 3.284529, mean_absolute_error: 10.312313, mean_q: 20.166819\n",
      " 4567/5000: episode: 167, duration: 0.760s, episode steps: 59, steps per second: 78, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.140 [-1.078, 0.402], loss: 3.710407, mean_absolute_error: 10.471150, mean_q: 20.435383\n",
      " 4619/5000: episode: 168, duration: 0.508s, episode steps: 52, steps per second: 102, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.136 [-1.047, 0.439], loss: 3.974083, mean_absolute_error: 10.541183, mean_q: 20.473164\n",
      " 4671/5000: episode: 169, duration: 0.427s, episode steps: 52, steps per second: 122, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.189 [-1.101, 0.388], loss: 3.244880, mean_absolute_error: 10.564553, mean_q: 20.638742\n",
      " 4727/5000: episode: 170, duration: 0.579s, episode steps: 56, steps per second: 97, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.135 [-1.072, 0.233], loss: 3.703835, mean_absolute_error: 10.691888, mean_q: 20.835318\n",
      " 4781/5000: episode: 171, duration: 0.361s, episode steps: 54, steps per second: 150, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.149 [-1.141, 0.355], loss: 4.224087, mean_absolute_error: 10.705315, mean_q: 20.802610\n",
      " 4844/5000: episode: 172, duration: 0.331s, episode steps: 63, steps per second: 190, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.157 [-1.234, 0.427], loss: 3.191108, mean_absolute_error: 10.818394, mean_q: 21.162336\n",
      " 4909/5000: episode: 173, duration: 0.468s, episode steps: 65, steps per second: 139, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.254 [-1.597, 0.245], loss: 3.902056, mean_absolute_error: 10.809274, mean_q: 21.105810\n",
      " 4996/5000: episode: 174, duration: 0.530s, episode steps: 87, steps per second: 164, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.211 [-1.691, 0.398], loss: 3.523658, mean_absolute_error: 10.896447, mean_q: 21.257612\n",
      "done, took 90.672 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb25027b70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the epsilon greedy policy improvment\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit = 50000, window_length = 1)\n",
    "dqn = DQNAgent(model = model, nb_actions = nb_actions, memory = memory, nb_steps_warmup = 10, target_model_update = 1e-2, policy = policy)\n",
    "dqn.compile(Adam(lr = 1e-3), metrics = ['mae'])\n",
    "dqn.fit(env, nb_steps = 5000, visualize = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 66.000, steps: 66\n",
      "Episode 2: reward: 66.000, steps: 66\n",
      "Episode 3: reward: 67.000, steps: 67\n",
      "Episode 4: reward: 81.000, steps: 81\n",
      "Episode 5: reward: 74.000, steps: 74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb222359e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes = 5, visualize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f0aecb7deac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithmic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Env' is not defined"
     ]
    }
   ],
   "source": [
    "Env.algorithmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
